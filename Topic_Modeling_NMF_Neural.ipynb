{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkp74/Topic_Modelling/blob/main/Topic_Modeling_NMF_Neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaaX0O6lVIH7"
      },
      "source": [
        "# Imports and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsR1lD01GuvE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.utils.extmath import safe_sparse_dot\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR6vyL24BLYn"
      },
      "outputs": [],
      "source": [
        "# Define the custom NMF-like neural network layer\n",
        "def custom_nmf_layer(input_layer, n_components):\n",
        "    W = Dense(n_components, activation='relu')(input_layer)\n",
        "    H = Dense(n_components, activation='relu', use_bias=False)(input_layer)\n",
        "    return W, H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlzxXa69FI7k"
      },
      "outputs": [],
      "source": [
        "def random_initialization(A, rank):\n",
        "    number_of_documents = A.shape[0]\n",
        "    number_of_terms = A.shape[1]\n",
        "    W = np.random.uniform(1, 2, (number_of_documents, rank))\n",
        "    H = np.random.uniform(1, 2, (rank, number_of_terms))\n",
        "    return W, H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuxqHbKP__Wq"
      },
      "outputs": [],
      "source": [
        "def nndsvd_initialization(A, rank):\n",
        "    u, s, v = np.linalg.svd(A, full_matrices=False)\n",
        "    v = v.T\n",
        "    w = np.zeros((A.shape[0], rank))\n",
        "    h = np.zeros((rank, A.shape[1]))\n",
        "\n",
        "    w[:, 0] = np.sqrt(s[0]) * np.abs(u[:, 0])\n",
        "    h[0, :] = np.sqrt(s[0]) * np.abs(v[:, 0].T)\n",
        "\n",
        "    for i in range(1, rank):\n",
        "        ui = u[:, i]\n",
        "        vi = v[:, i]\n",
        "        ui_pos = (ui >= 0) * ui\n",
        "        ui_neg = (ui < 0) * -ui\n",
        "        vi_pos = (vi >= 0) * vi\n",
        "        vi_neg = (vi < 0) * -vi\n",
        "\n",
        "        ui_pos_norm = np.linalg.norm(ui_pos, 2)\n",
        "        ui_neg_norm = np.linalg.norm(ui_neg, 2)\n",
        "        vi_pos_norm = np.linalg.norm(vi_pos, 2)\n",
        "        vi_neg_norm = np.linalg.norm(vi_neg, 2)\n",
        "\n",
        "        norm_pos = ui_pos_norm * vi_pos_norm\n",
        "        norm_neg = ui_neg_norm * vi_neg_norm\n",
        "\n",
        "        if norm_pos >= norm_neg:\n",
        "            w[:, i] = np.sqrt(s[i] * norm_pos) / ui_pos_norm * ui_pos\n",
        "            h[i, :] = np.sqrt(s[i] * norm_pos) / vi_pos_norm * vi_pos.T\n",
        "        else:\n",
        "            w[:, i] = np.sqrt(s[i] * norm_neg) / ui_neg_norm * ui_neg\n",
        "            h[i, :] = np.sqrt(s[i] * norm_neg) / vi_neg_norm * vi_neg.T\n",
        "\n",
        "    return w, h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfzHApLrUsWX"
      },
      "source": [
        "# Using Random Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBPK358CCR60"
      },
      "outputs": [],
      "source": [
        "def mu_method(A, k, max_iter, init_mode='random'):\n",
        "    # Initialize W and H\n",
        "    W, H = random_initialization(A, k)\n",
        "\n",
        "    norms = []\n",
        "    e = 1.0e-10\n",
        "\n",
        "    for n in range(max_iter):\n",
        "        # Update H\n",
        "        W_TA = W.T @ A\n",
        "        W_TWH = W.T @ W @ H + e\n",
        "\n",
        "        H = np.multiply(H, (W_TA / W_TWH))\n",
        "\n",
        "        # Update W\n",
        "        AH_T = A @ H.T\n",
        "        WHH_T =  W @ H @ H.T + e\n",
        "\n",
        "        W = np.multiply(W, (AH_T / WHH_T))\n",
        "\n",
        "        norm = np.linalg.norm(A - W @ H, 'fro')\n",
        "        norms.append(norm)\n",
        "\n",
        "    return W, H, norms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu5WkrsMGwyJ"
      },
      "outputs": [],
      "source": [
        "n_samples = 2000\n",
        "n_features = 1000\n",
        "n_components = 5\n",
        "n_top_words = 20\n",
        "batch_size = 128\n",
        "init = \"nndsvda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oks9zPz5G1lP",
        "outputId": "7aecd54c-cf0b-4748-eded-b87a313bc44f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the data\n",
        "print(\"Loading dataset...\")\n",
        "t0 = time()\n",
        "data, _ = fetch_20newsgroups(\n",
        "    shuffle=True,\n",
        "    random_state=1,\n",
        "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "    return_X_y=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUALyZgwG4mz",
        "outputId": "f69a5137-0276-4a11-9ebf-a79b64c78580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done in 1.449s.\n"
          ]
        }
      ],
      "source": [
        "data_samples = data[:n_samples]\n",
        "print(\"done in %0.3fs.\" % (time() - t0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnCGfvVmG7GB",
        "outputId": "5b325bff-a718-42e0-c4fd-42c312d563e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting tf-idf features for NMF-like neural network training...\n",
            "done in 0.410s.\n"
          ]
        }
      ],
      "source": [
        "# Use tf-idf features for training the NMF-like neural network\n",
        "print(\"Extracting tf-idf features for NMF-like neural network training...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\")\n",
        "t0 = time()\n",
        "tfidf = tfidf_vectorizer.fit_transform(data_samples).toarray().T\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJVua4rrG7lC"
      },
      "outputs": [],
      "source": [
        "# Call the mu_method for NMF\n",
        "A = tfidf.T  # Transpose the tfidf matrix to match the input shape\n",
        "W, H, norms = mu_method(A, n_components, max_iter=200, init_mode=init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDgR2zAqHAXT"
      },
      "outputs": [],
      "source": [
        "def extract_topics(W, H, feature_names, n_top_words):\n",
        "    num_topics = W.shape[1]\n",
        "\n",
        "    for topic_idx in range(num_topics):\n",
        "        # Ensure indices are within the valid range\n",
        "        top_features_ind = np.argsort(W[:, topic_idx])[::-1][:n_top_words]\n",
        "\n",
        "        # Ensure top_features_ind is within the valid range of feature_names\n",
        "        valid_top_features_ind = top_features_ind[top_features_ind < len(feature_names)]\n",
        "        top_features = feature_names[valid_top_features_ind]\n",
        "        weights = W[valid_top_features_ind, topic_idx]\n",
        "\n",
        "        print(f\"Topic {topic_idx + 1}:\")\n",
        "        for feature, weight in zip(top_features, weights):\n",
        "            print(f\"{feature}: {weight:.4f}\")\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcdEtLsNHA6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b9c0ac-5de2-48a5-d58e-7beda56a1bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1:\n",
            "mac: 23.3556\n",
            "difference: 21.3883\n",
            "ll: 16.4255\n",
            "serial: 16.4236\n",
            "oil: 15.9419\n",
            "project: 14.9202\n",
            "\n",
            "\n",
            "Topic 2:\n",
            "information: 16.0106\n",
            "shot: 13.5583\n",
            "book: 13.4951\n",
            "gov: 12.4691\n",
            "systems: 11.2476\n",
            "3d: 11.2385\n",
            "tv: 10.8619\n",
            "board: 10.3814\n",
            "\n",
            "\n",
            "Topic 3:\n",
            "size: 11.3294\n",
            "limited: 10.6152\n",
            "nasa: 10.2993\n",
            "changes: 9.9262\n",
            "armenia: 9.3184\n",
            "won: 9.0258\n",
            "research: 8.7738\n",
            "suppose: 8.6806\n",
            "\n",
            "\n",
            "Topic 4:\n",
            "43: 14.1276\n",
            "energy: 11.9571\n",
            "head: 11.2984\n",
            "jobs: 11.2523\n",
            "recent: 11.2057\n",
            "clear: 10.9800\n",
            "women: 10.7775\n",
            "\n",
            "\n",
            "Topic 5:\n",
            "years: 16.3171\n",
            "medical: 15.9220\n",
            "reply: 14.2778\n",
            "date: 14.2589\n",
            "output: 12.7142\n",
            "hard: 12.4516\n",
            "gives: 12.0153\n",
            "author: 11.9079\n",
            "defense: 11.8827\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract topics using the learned NMF-like components\n",
        "tfidf_feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "extract_topics(W, H, tfidf_feature_names, n_top_words = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phrJSC39DXYV"
      },
      "outputs": [],
      "source": [
        "def visualize_word_clouds(W, feature_names, n_top_words, output_dir=\"word_clouds\"):\n",
        "    # Create an output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    num_topics = W.shape[1]\n",
        "\n",
        "    for topic_idx in range(num_topics):\n",
        "        # Get the top words for the current topic\n",
        "        top_features_ind = np.argsort(W[:, topic_idx])[::-1][:n_top_words]\n",
        "\n",
        "        # Ensure the indices are within the valid range of feature_names\n",
        "        valid_top_features_ind = top_features_ind[top_features_ind < len(feature_names)]\n",
        "        top_features = [feature_names[i] for i in valid_top_features_ind]\n",
        "        weights = [W[i, topic_idx] for i in valid_top_features_ind]\n",
        "\n",
        "        # Create a dictionary of words and their weights for the word cloud\n",
        "        wordcloud_data = {top_features[i]: weights[i] for i in range(len(top_features))}\n",
        "\n",
        "        # Create and save the word cloud for the current topic\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(wordcloud_data)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'Topic {topic_idx + 1}')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Save the word cloud as an image\n",
        "        image_file = os.path.join(output_dir, f\"topic_{topic_idx + 1}_wordcloud.png\")\n",
        "        plt.savefig(image_file)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyHnxPRvDYEZ"
      },
      "outputs": [],
      "source": [
        "# Call the visualize_word_clouds function\n",
        "visualize_word_clouds(W, tfidf_feature_names, n_top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8gUHVwYGMp1"
      },
      "outputs": [],
      "source": [
        "def calculate_coherence(W, feature_names, texts, n_top_words):\n",
        "    num_topics = W.shape[1]\n",
        "\n",
        "    # Ensure valid indices and adjust n_top_words if needed\n",
        "    n_top_words = min(n_top_words, len(feature_names))\n",
        "\n",
        "    # Extract top words for each topic\n",
        "    top_words_per_topic = []\n",
        "    for topic_idx in range(num_topics):\n",
        "        top_features_ind = np.argsort(W[:, topic_idx])[::-1][:n_top_words]\n",
        "        top_words = [feature_names[i] for i in top_features_ind if i < len(feature_names)]\n",
        "        top_words_per_topic.append(top_words)\n",
        "\n",
        "    # Create a CountVectorizer to convert text to a bag of words\n",
        "    vectorizer = CountVectorizer(vocabulary=feature_names, binary=True)\n",
        "    bow_matrix = vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "    # Calculate co-occurrence matrix (word by word)\n",
        "    co_occurrence_matrix = np.dot(bow_matrix.T, bow_matrix)\n",
        "    np.fill_diagonal(co_occurrence_matrix, 0)\n",
        "\n",
        "    # Initialize coherence\n",
        "    coherence = 0.0\n",
        "\n",
        "    # Calculate coherence for each topic\n",
        "    for topic_words in top_words_per_topic:\n",
        "        topic_coherence = 0.0\n",
        "        for i in range(len(topic_words)):\n",
        "            for j in range(i + 1, len(topic_words)):\n",
        "                word_i, word_j = topic_words[i], topic_words[j]\n",
        "                if word_i in feature_names and word_j in feature_names:\n",
        "                    word_i_idx, word_j_idx = np.where(feature_names == word_i)[0], np.where(feature_names == word_j)[0]\n",
        "                    if len(word_i_idx) > 0 and len(word_j_idx) > 0:\n",
        "                        co_occurrences = co_occurrence_matrix[word_i_idx[0], word_j_idx[0]]\n",
        "                        word_i_freq = np.sum(bow_matrix[:, word_i_idx])\n",
        "                        word_j_freq = np.sum(bow_matrix[:, word_j_idx])\n",
        "\n",
        "                        # Compute Pointwise Mutual Information (PMI)\n",
        "                        pmi = np.log((co_occurrences * len(texts)) / (word_i_freq * word_j_freq) + 1e-10)\n",
        "                        topic_coherence += pmi\n",
        "\n",
        "        # Average over word pairs\n",
        "        topic_coherence /= len(topic_words)\n",
        "        coherence += topic_coherence\n",
        "\n",
        "    # Average over topics\n",
        "    coherence /= num_topics\n",
        "\n",
        "    return coherence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neLgEIXHKMOu",
        "outputId": "41e20b39-a740-4f4b-f028-a3bc59dd5cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence: -12.981491238355256\n"
          ]
        }
      ],
      "source": [
        "# Call the function to calculate coherence\n",
        "coherence = calculate_coherence(W, tfidf_feature_names, data_samples, n_top_words)\n",
        "print(f\"Coherence: {coherence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS520T-0Ux0u"
      },
      "source": [
        "# Using Nonnegative Double Singular Value Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXXwzLqfVC8X"
      },
      "outputs": [],
      "source": [
        "def mu_method(A, k, max_iter, init_mode='random'):\n",
        "    # Initialize W and H\n",
        "    W, H = nndsvd_initialization(A, k)\n",
        "\n",
        "    norms = []\n",
        "    e = 1.0e-10\n",
        "\n",
        "    for n in range(max_iter):\n",
        "        # Update H\n",
        "        W_TA = W.T @ A\n",
        "        W_TWH = W.T @ W @ H + e\n",
        "\n",
        "        H = np.multiply(H, (W_TA / W_TWH))\n",
        "\n",
        "        # Update W\n",
        "        AH_T = A @ H.T\n",
        "        WHH_T =  W @ H @ H.T + e\n",
        "\n",
        "        W = np.multiply(W, (AH_T / WHH_T))\n",
        "\n",
        "        norm = np.linalg.norm(A - W @ H, 'fro')\n",
        "        norms.append(norm)\n",
        "\n",
        "    return W, H, norms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1swg_xENVjVF"
      },
      "outputs": [],
      "source": [
        "# Call the mu_method for NMF\n",
        "A = tfidf.T  # Transpose the tfidf matrix to match the input shape\n",
        "W, H, norms = mu_method(A, n_components, max_iter=100, init_mode=init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6ASiivwVoQb"
      },
      "outputs": [],
      "source": [
        "def extract_topics(W, H, feature_names, n_top_words):\n",
        "    num_topics = W.shape[1]\n",
        "\n",
        "    for topic_idx in range(num_topics):\n",
        "        # Ensure indices are within the valid range\n",
        "        top_features_ind = np.argsort(W[:, topic_idx])[::-1][:n_top_words]\n",
        "\n",
        "        # Ensure top_features_ind is within the valid range of feature_names\n",
        "        valid_top_features_ind = top_features_ind[top_features_ind < len(feature_names)]\n",
        "        top_features = feature_names[valid_top_features_ind]\n",
        "        weights = W[valid_top_features_ind, topic_idx]\n",
        "\n",
        "        print(f\"Topic {topic_idx + 1}:\")\n",
        "        for feature, weight in zip(top_features, weights):\n",
        "            print(f\"{feature}: {weight:.4f}\")\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JudQfHHVy66",
        "outputId": "897b9366-678e-4966-9e10-5e4f6ef76e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1:\n",
            "size: 0.2262\n",
            "limited: 0.2051\n",
            "nasa: 0.1835\n",
            "armenia: 0.1792\n",
            "changes: 0.1722\n",
            "30: 0.1674\n",
            "looking: 0.1667\n",
            "suppose: 0.1652\n",
            "\n",
            "\n",
            "Topic 2:\n",
            "years: 0.2325\n",
            "medical: 0.2208\n",
            "date: 0.2157\n",
            "author: 0.2087\n",
            "output: 0.2061\n",
            "board: 0.1984\n",
            "hard: 0.1935\n",
            "basically: 0.1932\n",
            "given: 0.1847\n",
            "reply: 0.1825\n",
            "\n",
            "\n",
            "Topic 3:\n",
            "mac: 0.4521\n",
            "difference: 0.3767\n",
            "ll: 0.2897\n",
            "oil: 0.2881\n",
            "serial: 0.2859\n",
            "project: 0.2599\n",
            "\n",
            "\n",
            "Topic 4:\n",
            "recent: 0.2965\n",
            "clear: 0.2411\n",
            "head: 0.2204\n",
            "jobs: 0.2139\n",
            "connector: 0.2116\n",
            "43: 0.2038\n",
            "areas: 0.1942\n",
            "war: 0.1940\n",
            "food: 0.1865\n",
            "women: 0.1825\n",
            "17: 0.1823\n",
            "\n",
            "\n",
            "Topic 5:\n",
            "science: 0.2775\n",
            "hours: 0.2755\n",
            "best: 0.2579\n",
            "results: 0.2532\n",
            "scsi: 0.2496\n",
            "cost: 0.2448\n",
            "president: 0.2402\n",
            "wouldn: 0.2300\n",
            "jesus: 0.2197\n",
            "access: 0.2164\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract topics using the learned NMF-like components\n",
        "tfidf_feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "extract_topics(W, H, tfidf_feature_names, n_top_words = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGK3GTjeV2Bq"
      },
      "outputs": [],
      "source": [
        "def visualize_word_clouds(W, feature_names, n_top_words, output_dir=\"word_clouds_svd\"):\n",
        "    # Create an output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    num_topics = W.shape[1]\n",
        "\n",
        "    for topic_idx in range(num_topics):\n",
        "        # Get the top words for the current topic\n",
        "        top_features_ind = np.argsort(W[:, topic_idx])[::-1][:n_top_words]\n",
        "\n",
        "        # Ensure the indices are within the valid range of feature_names\n",
        "        valid_top_features_ind = top_features_ind[top_features_ind < len(feature_names)]\n",
        "        top_features = [feature_names[i] for i in valid_top_features_ind]\n",
        "        weights = [W[i, topic_idx] for i in valid_top_features_ind]\n",
        "\n",
        "        # Create a dictionary of words and their weights for the word cloud\n",
        "        wordcloud_data = {top_features[i]: weights[i] for i in range(len(top_features))}\n",
        "\n",
        "        # Create and save the word cloud for the current topic\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(wordcloud_data)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'Topic {topic_idx + 1}')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Save the word cloud as an image\n",
        "        image_file = os.path.join(output_dir, f\"topic_{topic_idx + 1}_wordcloud.png\")\n",
        "        plt.savefig(image_file)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4OJDFGmV6X6"
      },
      "outputs": [],
      "source": [
        "# Call the visualize_word_clouds function\n",
        "visualize_word_clouds(W, tfidf_feature_names, n_top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPjWY10WWCKQ"
      },
      "outputs": [],
      "source": [
        "def calculate_coherence(W, feature_names, texts, n_top_words):\n",
        "    num_topics = W.shape[1]\n",
        "\n",
        "    # Ensure valid indices and adjust n_top_words if needed\n",
        "    n_top_words = min(n_top_words, len(feature_names))\n",
        "\n",
        "    # Extract top words for each topic\n",
        "    top_words_per_topic = []\n",
        "    for topic_idx in range(num_topics):\n",
        "        top_features_ind = np.argsort(W[:, topic_idx])[::-1][:n_top_words]\n",
        "        top_words = [feature_names[i] for i in top_features_ind if i < len(feature_names)]\n",
        "        top_words_per_topic.append(top_words)\n",
        "\n",
        "    # Create a CountVectorizer to convert text to a bag of words\n",
        "    vectorizer = CountVectorizer(vocabulary=feature_names, binary=True)\n",
        "    bow_matrix = vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "    # Calculate co-occurrence matrix (word by word)\n",
        "    co_occurrence_matrix = np.dot(bow_matrix.T, bow_matrix)\n",
        "    np.fill_diagonal(co_occurrence_matrix, 0)\n",
        "\n",
        "    # Initialize coherence\n",
        "    coherence = 0.0\n",
        "\n",
        "    # Calculate coherence for each topic\n",
        "    for topic_words in top_words_per_topic:\n",
        "        topic_coherence = 0.0\n",
        "        for i in range(len(topic_words)):\n",
        "            for j in range(i + 1, len(topic_words)):\n",
        "                word_i, word_j = topic_words[i], topic_words[j]\n",
        "                if word_i in feature_names and word_j in feature_names:\n",
        "                    word_i_idx, word_j_idx = np.where(feature_names == word_i)[0], np.where(feature_names == word_j)[0]\n",
        "                    if len(word_i_idx) > 0 and len(word_j_idx) > 0:\n",
        "                        co_occurrences = co_occurrence_matrix[word_i_idx[0], word_j_idx[0]]\n",
        "                        word_i_freq = np.sum(bow_matrix[:, word_i_idx])\n",
        "                        word_j_freq = np.sum(bow_matrix[:, word_j_idx])\n",
        "\n",
        "                        # Compute Pointwise Mutual Information (PMI)\n",
        "                        pmi = np.log((co_occurrences * len(texts)) / (word_i_freq * word_j_freq) + 1e-10)\n",
        "                        topic_coherence += pmi\n",
        "\n",
        "        # Average over word pairs\n",
        "        topic_coherence /= len(topic_words)\n",
        "        coherence += topic_coherence\n",
        "\n",
        "    # Average over topics\n",
        "    coherence /= num_topics\n",
        "\n",
        "    return coherence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsGCoaooWGAZ",
        "outputId": "d22da8c8-f838-4881-c8bb-a9f4c5f5464c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence: -13.802921864496625\n"
          ]
        }
      ],
      "source": [
        "# Call the function to calculate coherence\n",
        "coherence = calculate_coherence(W, tfidf_feature_names, data_samples, n_top_words)\n",
        "print(f\"Coherence: {coherence}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyGM/MWglbXs2tioUGOIka",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}